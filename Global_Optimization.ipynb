{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy Optimize Global\n",
    "\n",
    "## Global Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the packages for the example\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x :  1/50*x[0]**2 - np.cos(x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27d9b834128>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = np.arange(-6*np.pi,6*np.pi,.1)\n",
    "\n",
    "plt.plot(x0,f([x0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds=[(-6*np.pi,6*np.pi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: -1.0\n",
       "    funl: array([-1.])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 6\n",
       "     nit: 2\n",
       "   nlfev: 3\n",
       "   nlhev: 0\n",
       "   nljev: 1\n",
       " success: True\n",
       "       x: array([0.])\n",
       "      xl: array([[0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize.shgo(f,bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: -0.9951923133916076\n",
       "    funl: array([-0.99519231])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 15\n",
       "     nit: 2\n",
       "   nlfev: 12\n",
       "   nlhev: 0\n",
       "   nljev: 4\n",
       " success: True\n",
       "       x: array([0.48076808])\n",
       "      xl: array([[0.48076808]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = lambda x, a :  1/50*x[0]**2 - np.cos(x[0]-a)\n",
    "optimize.shgo(g,bounds,args=(0.5,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       " \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasinhopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mniter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstepsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mminimizer_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtake_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0maccept_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdisp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mniter_success\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Find the global minimum of a function using the basin-hopping algorithm\n",
       "\n",
       "Basin-hopping is a two-phase method that combines a global stepping\n",
       "algorithm with local minimization at each step.  Designed to mimic\n",
       "the natural process of energy minimization of clusters of atoms, it works\n",
       "well for similar problems with \"funnel-like, but rugged\" energy landscapes\n",
       "[5]_.\n",
       "\n",
       "As the step-taking, step acceptance, and minimization methods are all\n",
       "customizable, this function can also be used to implement other two-phase\n",
       "methods.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "func : callable ``f(x, *args)``\n",
       "    Function to be optimized.  ``args`` can be passed as an optional item\n",
       "    in the dict ``minimizer_kwargs``\n",
       "x0 : array_like\n",
       "    Initial guess.\n",
       "niter : integer, optional\n",
       "    The number of basin-hopping iterations\n",
       "T : float, optional\n",
       "    The \"temperature\" parameter for the accept or reject criterion.  Higher\n",
       "    \"temperatures\" mean that larger jumps in function value will be\n",
       "    accepted.  For best results ``T`` should be comparable to the\n",
       "    separation (in function value) between local minima.\n",
       "stepsize : float, optional\n",
       "    Maximum step size for use in the random displacement.\n",
       "minimizer_kwargs : dict, optional\n",
       "    Extra keyword arguments to be passed to the local minimizer\n",
       "    ``scipy.optimize.minimize()`` Some important options could be:\n",
       "\n",
       "        method : str\n",
       "            The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
       "        args : tuple\n",
       "            Extra arguments passed to the objective function (``func``) and\n",
       "            its derivatives (Jacobian, Hessian).\n",
       "\n",
       "take_step : callable ``take_step(x)``, optional\n",
       "    Replace the default step-taking routine with this routine.  The default\n",
       "    step-taking routine is a random displacement of the coordinates, but\n",
       "    other step-taking algorithms may be better for some systems.\n",
       "    ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
       "    If this attribute exists, then ``basinhopping`` will adjust\n",
       "    ``take_step.stepsize`` in order to try to optimize the global minimum\n",
       "    search.\n",
       "accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
       "    Define a test which will be used to judge whether or not to accept the\n",
       "    step.  This will be used in addition to the Metropolis test based on\n",
       "    \"temperature\" ``T``.  The acceptable return values are True,\n",
       "    False, or ``\"force accept\"``. If any of the tests return False\n",
       "    then the step is rejected. If the latter, then this will override any\n",
       "    other tests in order to accept the step. This can be used, for example,\n",
       "    to forcefully escape from a local minimum that ``basinhopping`` is\n",
       "    trapped in.\n",
       "callback : callable, ``callback(x, f, accept)``, optional\n",
       "    A callback function which will be called for all minima found.  ``x``\n",
       "    and ``f`` are the coordinates and function value of the trial minimum,\n",
       "    and ``accept`` is whether or not that minimum was accepted.  This can\n",
       "    be used, for example, to save the lowest N minima found.  Also,\n",
       "    ``callback`` can be used to specify a user defined stop criterion by\n",
       "    optionally returning True to stop the ``basinhopping`` routine.\n",
       "interval : integer, optional\n",
       "    interval for how often to update the ``stepsize``\n",
       "disp : bool, optional\n",
       "    Set to True to print status messages\n",
       "niter_success : integer, optional\n",
       "    Stop the run if the global minimum candidate remains the same for this\n",
       "    number of iterations.\n",
       "seed : int or `np.random.RandomState`, optional\n",
       "    If `seed` is not specified the `np.RandomState` singleton is used.\n",
       "    If `seed` is an int, a new `np.random.RandomState` instance is used,\n",
       "    seeded with seed.\n",
       "    If `seed` is already a `np.random.RandomState instance`, then that\n",
       "    `np.random.RandomState` instance is used.\n",
       "    Specify `seed` for repeatable minimizations. The random numbers\n",
       "    generated with this seed only affect the default Metropolis\n",
       "    `accept_test` and the default `take_step`. If you supply your own\n",
       "    `take_step` and `accept_test`, and these functions use random\n",
       "    number generation, then those functions are responsible for the state\n",
       "    of their random number generator.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "res : OptimizeResult\n",
       "    The optimization result represented as a ``OptimizeResult`` object.\n",
       "    Important attributes are: ``x`` the solution array, ``fun`` the value\n",
       "    of the function at the solution, and ``message`` which describes the\n",
       "    cause of the termination. The ``OptimizeResult`` object returned by the\n",
       "    selected minimizer at the lowest minimum is also contained within this\n",
       "    object and can be accessed through the ``lowest_optimization_result``\n",
       "    attribute.  See `OptimizeResult` for a description of other attributes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "minimize :\n",
       "    The local minimization function called once for each basinhopping step.\n",
       "    ``minimizer_kwargs`` is passed to this routine.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Basin-hopping is a stochastic algorithm which attempts to find the global\n",
       "minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
       "[4]_.  The algorithm in its current form was described by David Wales and\n",
       "Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
       "\n",
       "The algorithm is iterative with each cycle composed of the following\n",
       "features\n",
       "\n",
       "1) random perturbation of the coordinates\n",
       "\n",
       "2) local minimization\n",
       "\n",
       "3) accept or reject the new coordinates based on the minimized function\n",
       "   value\n",
       "\n",
       "The acceptance test used here is the Metropolis criterion of standard Monte\n",
       "Carlo algorithms, although there are many other possibilities [3]_.\n",
       "\n",
       "This global minimization method has been shown to be extremely efficient\n",
       "for a wide variety of problems in physics and chemistry.  It is\n",
       "particularly useful when the function has many minima separated by large\n",
       "barriers. See the Cambridge Cluster Database\n",
       "http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
       "that have been optimized primarily using basin-hopping.  This database\n",
       "includes minimization problems exceeding 300 degrees of freedom.\n",
       "\n",
       "See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
       "a Fortran implementation of basin-hopping.  This implementation has many\n",
       "different variations of the procedure described above, including more\n",
       "advanced step taking algorithms and alternate acceptance criterion.\n",
       "\n",
       "For stochastic global optimization there is no way to determine if the true\n",
       "global minimum has actually been found. Instead, as a consistency check,\n",
       "the algorithm can be run from a number of different random starting points\n",
       "to ensure the lowest minimum found in each example has converged to the\n",
       "global minimum.  For this reason ``basinhopping`` will by default simply\n",
       "run for the number of iterations ``niter`` and return the lowest minimum\n",
       "found.  It is left to the user to ensure that this is in fact the global\n",
       "minimum.\n",
       "\n",
       "Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
       "depends on the problem being solved.  The step is chosen uniformly in the\n",
       "region from x0-stepsize to x0+stepsize, in each dimension.  Ideally it\n",
       "should be comparable to the typical separation (in argument values) between\n",
       "local minima of the function being optimized.  ``basinhopping`` will, by\n",
       "default, adjust ``stepsize`` to find an optimal value, but this may take\n",
       "many iterations.  You will get quicker results if you set a sensible\n",
       "initial value for ``stepsize``.\n",
       "\n",
       "Choosing ``T``: The parameter ``T`` is the \"temperature\" used in the\n",
       "Metropolis criterion.  Basinhopping steps are always accepted if\n",
       "``func(xnew) < func(xold)``.  Otherwise, they are accepted with\n",
       "probability::\n",
       "\n",
       "    exp( -(func(xnew) - func(xold)) / T )\n",
       "\n",
       "So, for best results, ``T`` should to be comparable to the typical\n",
       "difference (in function values) between local minima.  (The height of\n",
       "\"walls\" between local minima is irrelevant.)\n",
       "\n",
       "If ``T`` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all\n",
       "steps that increase energy are rejected.\n",
       "\n",
       ".. versionadded:: 0.12.0\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
       "    Cambridge, UK.\n",
       ".. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
       "    the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
       "    110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
       ".. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
       "    multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
       "    1987, 84, 6611.\n",
       ".. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
       "    crystals, and biomolecules, Science, 1999, 285, 1368.\n",
       ".. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as\n",
       "    a General and Versatile Optimization Framework for the Characterization\n",
       "    of Biological Macromolecules, Advances in Artificial Intelligence,\n",
       "    Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`\n",
       "\n",
       "Examples\n",
       "--------\n",
       "The following example is a one-dimensional minimization problem,  with many\n",
       "local minima superimposed on a parabola.\n",
       "\n",
       ">>> from scipy.optimize import basinhopping\n",
       ">>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
       ">>> x0=[1.]\n",
       "\n",
       "Basinhopping, internally, uses a local minimization algorithm.  We will use\n",
       "the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
       "use and how to set up that minimizer.  This parameter will be passed to\n",
       "``scipy.optimize.minimize()``.\n",
       "\n",
       ">>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
       ">>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
       "...                    niter=200)\n",
       ">>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
       "global minimum: x = -0.1951, f(x0) = -1.0009\n",
       "\n",
       "Next consider a two-dimensional minimization problem. Also, this time we\n",
       "will use gradient information to significantly speed up the search.\n",
       "\n",
       ">>> def func2d(x):\n",
       "...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
       "...                                                            0.2) * x[0]\n",
       "...     df = np.zeros(2)\n",
       "...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
       "...     df[1] = 2. * x[1] + 0.2\n",
       "...     return f, df\n",
       "\n",
       "We'll also use a different local minimization algorithm.  Also we must tell\n",
       "the minimizer that our function returns both energy and gradient (jacobian)\n",
       "\n",
       ">>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
       ">>> x0 = [1.0, 1.0]\n",
       ">>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
       "...                    niter=200)\n",
       ">>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
       "...                                                           ret.x[1],\n",
       "...                                                           ret.fun))\n",
       "global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
       "\n",
       "\n",
       "Here is an example using a custom step-taking routine.  Imagine you want\n",
       "the first coordinate to take larger steps than the rest of the coordinates.\n",
       "This can be implemented like so:\n",
       "\n",
       ">>> class MyTakeStep(object):\n",
       "...    def __init__(self, stepsize=0.5):\n",
       "...        self.stepsize = stepsize\n",
       "...    def __call__(self, x):\n",
       "...        s = self.stepsize\n",
       "...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
       "...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
       "...        return x\n",
       "\n",
       "Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
       "of ``stepsize`` to optimize the search.  We'll use the same 2-D function as\n",
       "before\n",
       "\n",
       ">>> mytakestep = MyTakeStep()\n",
       ">>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
       "...                    niter=200, take_step=mytakestep)\n",
       ">>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
       "...                                                           ret.x[1],\n",
       "...                                                           ret.fun))\n",
       "global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
       "\n",
       "\n",
       "Now let's do an example using a custom callback function which prints the\n",
       "value of every minimum found\n",
       "\n",
       ">>> def print_fun(x, f, accepted):\n",
       "...         print(\"at minimum %.4f accepted %d\" % (f, int(accepted)))\n",
       "\n",
       "We'll run it for only 10 basinhopping steps this time.\n",
       "\n",
       ">>> np.random.seed(1)\n",
       ">>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
       "...                    niter=10, callback=print_fun)\n",
       "at minimum 0.4159 accepted 1\n",
       "at minimum -0.9073 accepted 1\n",
       "at minimum -0.1021 accepted 1\n",
       "at minimum -0.1021 accepted 1\n",
       "at minimum 0.9102 accepted 1\n",
       "at minimum 0.9102 accepted 1\n",
       "at minimum 2.2945 accepted 0\n",
       "at minimum -0.1021 accepted 1\n",
       "at minimum -1.0109 accepted 1\n",
       "at minimum -1.0109 accepted 1\n",
       "\n",
       "\n",
       "The minimum at -1.0109 is actually the global minimum, found already on the\n",
       "8th iteration.\n",
       "\n",
       "Now let's implement bounds on the problem using a custom ``accept_test``:\n",
       "\n",
       ">>> class MyBounds(object):\n",
       "...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
       "...         self.xmax = np.array(xmax)\n",
       "...         self.xmin = np.array(xmin)\n",
       "...     def __call__(self, **kwargs):\n",
       "...         x = kwargs[\"x_new\"]\n",
       "...         tmax = bool(np.all(x <= self.xmax))\n",
       "...         tmin = bool(np.all(x >= self.xmin))\n",
       "...         return tmax and tmin\n",
       "\n",
       ">>> mybounds = MyBounds()\n",
       ">>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
       "...                    niter=10, accept_test=mybounds)\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\chris\\.julia\\conda\\3\\lib\\site-packages\\scipy\\optimize\\_basinhopping.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? optimize.basinhopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       " \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshgo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconstraints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mminimizer_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msampling_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'simplicial'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Finds the global minimum of a function using SHG optimization.\n",
       "\n",
       "SHGO stands for \"simplicial homology global optimization\".\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "func : callable\n",
       "    The objective function to be minimized.  Must be in the form\n",
       "    ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
       "    and ``args`` is a tuple of any additional fixed parameters needed to\n",
       "    completely specify the function.\n",
       "bounds : sequence\n",
       "    Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
       "    defining the lower and upper bounds for the optimizing argument of\n",
       "    `func`. It is required to have ``len(bounds) == len(x)``.\n",
       "    ``len(bounds)`` is used to determine the number of parameters in ``x``.\n",
       "    Use ``None`` for one of min or max when there is no bound in that\n",
       "    direction. By default bounds are ``(None, None)``.\n",
       "args : tuple, optional\n",
       "    Any additional fixed parameters needed to completely specify the\n",
       "    objective function.\n",
       "constraints : dict or sequence of dict, optional\n",
       "    Constraints definition.\n",
       "    Function(s) ``R**n`` in the form::\n",
       "\n",
       "        g(x) <= 0 applied as g : R^n -> R^m\n",
       "        h(x) == 0 applied as h : R^n -> R^p\n",
       "\n",
       "    Each constraint is defined in a dictionary with fields:\n",
       "\n",
       "        type : str\n",
       "            Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
       "        fun : callable\n",
       "            The function defining the constraint.\n",
       "        jac : callable, optional\n",
       "            The Jacobian of `fun` (only for SLSQP).\n",
       "        args : sequence, optional\n",
       "            Extra arguments to be passed to the function and Jacobian.\n",
       "\n",
       "    Equality constraint means that the constraint function result is to\n",
       "    be zero whereas inequality means that it is to be non-negative.\n",
       "    Note that COBYLA only supports inequality constraints.\n",
       "\n",
       "    .. note::\n",
       "\n",
       "       Only the COBYLA and SLSQP local minimize methods currently\n",
       "       support constraint arguments. If the ``constraints`` sequence\n",
       "       used in the local optimization problem is not defined in\n",
       "       ``minimizer_kwargs`` and a constrained method is used then the\n",
       "       global ``constraints`` will be used.\n",
       "       (Defining a ``constraints`` sequence in ``minimizer_kwargs``\n",
       "       means that ``constraints`` will not be added so if equality\n",
       "       constraints and so forth need to be added then the inequality\n",
       "       functions in ``constraints`` need to be added to\n",
       "       ``minimizer_kwargs`` too).\n",
       "\n",
       "n : int, optional\n",
       "    Number of sampling points used in the construction of the simplicial\n",
       "    complex. Note that this argument is only used for ``sobol`` and other\n",
       "    arbitrary `sampling_methods`.\n",
       "iters : int, optional\n",
       "    Number of iterations used in the construction of the simplicial complex.\n",
       "callback : callable, optional\n",
       "    Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
       "    current parameter vector.\n",
       "minimizer_kwargs : dict, optional\n",
       "    Extra keyword arguments to be passed to the minimizer\n",
       "    ``scipy.optimize.minimize`` Some important options could be:\n",
       "\n",
       "        * method : str\n",
       "            The minimization method (e.g. ``SLSQP``).\n",
       "        * args : tuple\n",
       "            Extra arguments passed to the objective function (``func``) and\n",
       "            its derivatives (Jacobian, Hessian).\n",
       "        * options : dict, optional\n",
       "            Note that by default the tolerance is specified as\n",
       "            ``{ftol: 1e-12}``\n",
       "\n",
       "options : dict, optional\n",
       "    A dictionary of solver options. Many of the options specified for the\n",
       "    global routine are also passed to the scipy.optimize.minimize routine.\n",
       "    The options that are also passed to the local routine are marked with\n",
       "    \"(L)\".\n",
       "\n",
       "    Stopping criteria, the algorithm will terminate if any of the specified\n",
       "    criteria are met. However, the default algorithm does not require any to\n",
       "    be specified:\n",
       "\n",
       "    * maxfev : int (L)\n",
       "        Maximum number of function evaluations in the feasible domain.\n",
       "        (Note only methods that support this option will terminate\n",
       "        the routine at precisely exact specified value. Otherwise the\n",
       "        criterion will only terminate during a global iteration)\n",
       "    * f_min\n",
       "        Specify the minimum objective function value, if it is known.\n",
       "    * f_tol : float\n",
       "        Precision goal for the value of f in the stopping\n",
       "        criterion. Note that the global routine will also\n",
       "        terminate if a sampling point in the global routine is\n",
       "        within this tolerance.\n",
       "    * maxiter : int\n",
       "        Maximum number of iterations to perform.\n",
       "    * maxev : int\n",
       "        Maximum number of sampling evaluations to perform (includes\n",
       "        searching in infeasible points).\n",
       "    * maxtime : float\n",
       "        Maximum processing runtime allowed\n",
       "    * minhgrd : int\n",
       "        Minimum homology group rank differential. The homology group of the\n",
       "        objective function is calculated (approximately) during every\n",
       "        iteration. The rank of this group has a one-to-one correspondence\n",
       "        with the number of locally convex subdomains in the objective\n",
       "        function (after adequate sampling points each of these subdomains\n",
       "        contain a unique global minimum). If the difference in the hgr is 0\n",
       "        between iterations for ``maxhgrd`` specified iterations the\n",
       "        algorithm will terminate.\n",
       "\n",
       "    Objective function knowledge:\n",
       "\n",
       "    * symmetry : bool\n",
       "        Specify True if the objective function contains symmetric variables.\n",
       "        The search space (and therefore performance) is decreased by O(n!).\n",
       "\n",
       "    * jac : bool or callable, optional\n",
       "        Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
       "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If ``jac`` is a\n",
       "        boolean and is True, ``fun`` is assumed to return the gradient along\n",
       "        with the objective function. If False, the gradient will be\n",
       "        estimated numerically. ``jac`` can also be a callable returning the\n",
       "        gradient of the objective. In this case, it must accept the same\n",
       "        arguments as ``fun``. (Passed to `scipy.optimize.minmize` automatically)\n",
       "\n",
       "    * hess, hessp : callable, optional\n",
       "        Hessian (matrix of second-order derivatives) of objective function\n",
       "        or Hessian of objective function times an arbitrary vector p.\n",
       "        Only for Newton-CG, dogleg, trust-ncg. Only one of ``hessp`` or\n",
       "        ``hess`` needs to be given. If ``hess`` is provided, then\n",
       "        ``hessp`` will be ignored. If neither ``hess`` nor ``hessp`` is\n",
       "        provided, then the Hessian product will be approximated using\n",
       "        finite differences on ``jac``. ``hessp`` must compute the Hessian\n",
       "        times an arbitrary vector. (Passed to `scipy.optimize.minmize`\n",
       "        automatically)\n",
       "\n",
       "    Algorithm settings:\n",
       "\n",
       "    * minimize_every_iter : bool\n",
       "        If True then promising global sampling points will be passed to a\n",
       "        local minimisation routine every iteration. If False then only the\n",
       "        final minimiser pool will be run. Defaults to False.\n",
       "    * local_iter : int\n",
       "        Only evaluate a few of the best minimiser pool candidates every\n",
       "        iteration. If False all potential points are passed to the local\n",
       "        minimisation routine.\n",
       "    * infty_constraints: bool\n",
       "        If True then any sampling points generated which are outside will\n",
       "        the feasible domain will be saved and given an objective function\n",
       "        value of ``inf``. If False then these points will be discarded.\n",
       "        Using this functionality could lead to higher performance with\n",
       "        respect to function evaluations before the global minimum is found,\n",
       "        specifying False will use less memory at the cost of a slight\n",
       "        decrease in performance. Defaults to True.\n",
       "\n",
       "    Feedback:\n",
       "\n",
       "    * disp : bool (L)\n",
       "        Set to True to print convergence messages.\n",
       "\n",
       "sampling_method : str or function, optional\n",
       "    Current built in sampling method options are ``sobol`` and\n",
       "    ``simplicial``. The default ``simplicial`` uses less memory and provides\n",
       "    the theoretical guarantee of convergence to the global minimum in finite\n",
       "    time. The ``sobol`` method is faster in terms of sampling point\n",
       "    generation at the cost of higher memory resources and the loss of\n",
       "    guaranteed convergence. It is more appropriate for most \"easier\"\n",
       "    problems where the convergence is relatively fast.\n",
       "    User defined sampling functions must accept two arguments of ``n``\n",
       "    sampling points of dimension ``dim`` per call and output an array of\n",
       "    sampling points with shape `n x dim`. \n",
       "\n",
       "Returns\n",
       "-------\n",
       "res : OptimizeResult\n",
       "    The optimization result represented as a `OptimizeResult` object.\n",
       "    Important attributes are:\n",
       "    ``x`` the solution array corresponding to the global minimum,\n",
       "    ``fun`` the function output at the global solution,\n",
       "    ``xl`` an ordered list of local minima solutions,\n",
       "    ``funl`` the function output at the corresponding local solutions,\n",
       "    ``success`` a Boolean flag indicating if the optimizer exited\n",
       "    successfully,\n",
       "    ``message`` which describes the cause of the termination,\n",
       "    ``nfev`` the total number of objective function evaluations including\n",
       "    the sampling calls,\n",
       "    ``nlfev`` the total number of objective function evaluations\n",
       "    culminating from all local search optimisations,\n",
       "    ``nit`` number of iterations performed by the global routine.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Global optimization using simplicial homology global optimisation [1]_.\n",
       "Appropriate for solving general purpose NLP and blackbox optimisation\n",
       "problems to global optimality (low dimensional problems).\n",
       "\n",
       "In general, the optimization problems are of the form::\n",
       "\n",
       "    minimize f(x) subject to\n",
       "\n",
       "    g_i(x) >= 0,  i = 1,...,m\n",
       "    h_j(x)  = 0,  j = 1,...,p\n",
       "\n",
       "where x is a vector of one or more variables. ``f(x)`` is the objective\n",
       "function ``R^n -> R``, ``g_i(x)`` are the inequality constraints, and\n",
       "``h_j(x)`` are the equality constraints.\n",
       "\n",
       "Optionally, the lower and upper bounds for each element in x can also be\n",
       "specified using the `bounds` argument.\n",
       "\n",
       "While most of the theoretical advantages of SHGO are only proven for when\n",
       "``f(x)`` is a Lipschitz smooth function. The algorithm is also proven to\n",
       "converge to the global optimum for the more general case where ``f(x)`` is\n",
       "non-continuous, non-convex and non-smooth, if the default sampling method\n",
       "is used [1]_.\n",
       "\n",
       "The local search method may be specified using the ``minimizer_kwargs``\n",
       "parameter which is passed on to ``scipy.optimize.minimize``. By default\n",
       "the ``SLSQP`` method is used. In general it is recommended to use the\n",
       "``SLSQP`` or ``COBYLA`` local minimization if inequality constraints\n",
       "are defined for the problem since the other methods do not use constraints.\n",
       "\n",
       "The ``sobol`` method points are generated using the Sobol (1967) [2]_\n",
       "sequence. The primitive polynomials and various sets of initial direction\n",
       "numbers for generating Sobol sequences is provided by [3]_ by Frances Kuo\n",
       "and Stephen Joe. The original program sobol.cc (MIT) is available and\n",
       "described at http://web.maths.unsw.edu.au/~fkuo/sobol/ translated to\n",
       "Python 3 by Carl Sandrock 2016-03-31.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] Endres, SC, Sandrock, C, Focke, WW (2018) \"A simplicial homology\n",
       "       algorithm for lipschitz optimisation\", Journal of Global Optimization.\n",
       ".. [2] Sobol, IM (1967) \"The distribution of points in a cube and the\n",
       "       approximate evaluation of integrals\", USSR Comput. Math. Math. Phys.\n",
       "       7, 86-112.\n",
       ".. [3] Joe, SW and Kuo, FY (2008) \"Constructing Sobol sequences with\n",
       "       better  two-dimensional projections\", SIAM J. Sci. Comput. 30,\n",
       "       2635-2654.\n",
       ".. [4] Hoch, W and Schittkowski, K (1981) \"Test examples for nonlinear\n",
       "       programming codes\", Lecture Notes in Economics and mathematical\n",
       "       Systems, 187. Springer-Verlag, New York.\n",
       "       http://www.ai7.uni-bayreuth.de/test_problem_coll.pdf\n",
       ".. [5] Wales, DJ (2015) \"Perspective: Insight into reaction coordinates and\n",
       "       dynamics from the potential energy landscape\",\n",
       "       Journal of Chemical Physics, 142(13), 2015.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "First consider the problem of minimizing the Rosenbrock function, `rosen`:\n",
       "\n",
       ">>> from scipy.optimize import rosen, shgo\n",
       ">>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
       ">>> result = shgo(rosen, bounds)\n",
       ">>> result.x, result.fun\n",
       "(array([ 1.,  1.,  1.,  1.,  1.]), 2.9203923741900809e-18)\n",
       "\n",
       "Note that bounds determine the dimensionality of the objective\n",
       "function and is therefore a required input, however you can specify\n",
       "empty bounds using ``None`` or objects like ``np.inf`` which will be\n",
       "converted to large float numbers.\n",
       "\n",
       ">>> bounds = [(None, None), ]*4\n",
       ">>> result = shgo(rosen, bounds)\n",
       ">>> result.x\n",
       "array([ 0.99999851,  0.99999704,  0.99999411,  0.9999882 ])\n",
       "\n",
       "Next we consider the Eggholder function, a problem with several local\n",
       "minima and one global minimum. We will demonstrate the use of arguments and\n",
       "the capabilities of `shgo`.\n",
       "(https://en.wikipedia.org/wiki/Test_functions_for_optimization)\n",
       "\n",
       ">>> def eggholder(x):\n",
       "...     return (-(x[1] + 47.0)\n",
       "...             * np.sin(np.sqrt(abs(x[0]/2.0 + (x[1] + 47.0))))\n",
       "...             - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47.0))))\n",
       "...             )\n",
       "...\n",
       ">>> bounds = [(-512, 512), (-512, 512)]\n",
       "\n",
       "`shgo` has two built-in low discrepancy sampling sequences.  First we will\n",
       "input 30 initial sampling points of the Sobol sequence:\n",
       "\n",
       ">>> result = shgo(eggholder, bounds, n=30, sampling_method='sobol')\n",
       ">>> result.x, result.fun\n",
       "(array([ 512.        ,  404.23180542]), -959.64066272085051)\n",
       "\n",
       "`shgo` also has a return for any other local minima that was found, these\n",
       " can be called using:\n",
       "\n",
       ">>> result.xl\n",
       "array([[ 512.        ,  404.23180542],\n",
       "       [ 283.07593402, -487.12566542],\n",
       "       [-294.66820039, -462.01964031],\n",
       "       [-105.87688985,  423.15324143],\n",
       "       [-242.97923629,  274.38032063],\n",
       "       [-506.25823477,    6.3131022 ],\n",
       "       [-408.71981195, -156.10117154],\n",
       "       [ 150.23210485,  301.31378508],\n",
       "       [  91.00922754, -391.28375925],\n",
       "       [ 202.8966344 , -269.38042147],\n",
       "       [ 361.66625957, -106.96490692],\n",
       "       [-219.40615102, -244.06022436],\n",
       "       [ 151.59603137, -100.61082677]])\n",
       "\n",
       ">>> result.funl\n",
       "array([-959.64066272, -718.16745962, -704.80659592, -565.99778097,\n",
       "       -559.78685655, -557.36868733, -507.87385942, -493.9605115 ,\n",
       "       -426.48799655, -421.15571437, -419.31194957, -410.98477763,\n",
       "       -202.53912972])\n",
       "\n",
       "These results are useful in applications where there are many global minima\n",
       "and the values of other global minima are desired or where the local minima\n",
       "can provide insight into the system (for example morphologies\n",
       "in physical chemistry [5]_).\n",
       "\n",
       "If we want to find a larger number of local minima, we can increase the\n",
       "number of sampling points or the number of iterations. We'll increase the\n",
       "number of sampling points to 60 and the number of iterations from the\n",
       "default of 1 to 5. This gives us 60 x 5 = 300 initial sampling points.\n",
       "\n",
       ">>> result_2 = shgo(eggholder, bounds, n=60, iters=5, sampling_method='sobol')\n",
       ">>> len(result.xl), len(result_2.xl)\n",
       "(13, 39)\n",
       "\n",
       "Note the difference between, e.g., ``n=180, iters=1`` and ``n=60, iters=3``.\n",
       "In the first case the promising points contained in the minimiser pool\n",
       "is processed only once. In the latter case it is processed every 60 sampling\n",
       "points for a total of 3 times.\n",
       "\n",
       "To demonstrate solving problems with non-linear constraints consider the\n",
       "following example from Hock and Schittkowski problem 73 (cattle-feed) [4]_::\n",
       "\n",
       "    minimize: f = 24.55 * x_1 + 26.75 * x_2 + 39 * x_3 + 40.50 * x_4\n",
       "\n",
       "    subject to: 2.3 * x_1 + 5.6 * x_2 + 11.1 * x_3 + 1.3 * x_4 - 5     >= 0,\n",
       "\n",
       "                12 * x_1 + 11.9 * x_2 + 41.8 * x_3 + 52.1 * x_4 - 21\n",
       "                    -1.645 * sqrt(0.28 * x_1**2 + 0.19 * x_2**2 +\n",
       "                                  20.5 * x_3**2 + 0.62 * x_4**2)       >= 0,\n",
       "\n",
       "                x_1 + x_2 + x_3 + x_4 - 1                              == 0,\n",
       "\n",
       "                1 >= x_i >= 0 for all i\n",
       "\n",
       "The approximate answer given in [4]_ is::\n",
       "\n",
       "    f([0.6355216, -0.12e-11, 0.3127019, 0.05177655]) = 29.894378\n",
       "\n",
       ">>> def f(x):  # (cattle-feed)\n",
       "...     return 24.55*x[0] + 26.75*x[1] + 39*x[2] + 40.50*x[3]\n",
       "...\n",
       ">>> def g1(x):\n",
       "...     return 2.3*x[0] + 5.6*x[1] + 11.1*x[2] + 1.3*x[3] - 5  # >=0\n",
       "...\n",
       ">>> def g2(x):\n",
       "...     return (12*x[0] + 11.9*x[1] +41.8*x[2] + 52.1*x[3] - 21\n",
       "...             - 1.645 * np.sqrt(0.28*x[0]**2 + 0.19*x[1]**2\n",
       "...                             + 20.5*x[2]**2 + 0.62*x[3]**2)\n",
       "...             ) # >=0\n",
       "...\n",
       ">>> def h1(x):\n",
       "...     return x[0] + x[1] + x[2] + x[3] - 1  # == 0\n",
       "...\n",
       ">>> cons = ({'type': 'ineq', 'fun': g1},\n",
       "...         {'type': 'ineq', 'fun': g2},\n",
       "...         {'type': 'eq', 'fun': h1})\n",
       ">>> bounds = [(0, 1.0),]*4\n",
       ">>> res = shgo(f, bounds, iters=3, constraints=cons)\n",
       ">>> res\n",
       "     fun: 29.894378159142136\n",
       "    funl: array([29.89437816])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 119\n",
       "     nit: 3\n",
       "   nlfev: 40\n",
       "   nlhev: 0\n",
       "   nljev: 5\n",
       " success: True\n",
       "       x: array([6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02])\n",
       "      xl: array([[6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02]])\n",
       "\n",
       ">>> g1(res.x), g2(res.x), h1(res.x)\n",
       "(-5.0626169922907138e-14, -2.9594104944408173e-12, 0.0)\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\chris\\.julia\\conda\\3\\lib\\site-packages\\scipy\\optimize\\_shgo.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? optimize.shgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
