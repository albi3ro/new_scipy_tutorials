{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy Optimize Module\n",
    "\n",
    "\n",
    "## Module Contents\n",
    "\n",
    "* Optimization\n",
    "    * Local\n",
    "        - [1D Optimization](./Optimization_1D.ipynb)\n",
    "        - [ND Optimization](./Optimization_ND.ipynb)\n",
    "        - [Linear Programming](./Linear_Prog.ipynb)\n",
    "    * Global Optimization\n",
    "        - [Brute](./Optimization_Global_brute.ipynb)\n",
    "        - [shgo](./Optimization_Global_shgo.ipynb)\n",
    "        - [Differential Evolution](./Optimization_Global_differential_evolution.ipynb)\n",
    "        - [Basin Hopping](./Optimization_Global_basinhopping.ipynb)\n",
    "        - [Dual Annealing](./Optimization_Global_dual_annealing.ipynb)\n",
    "* Root Finding\n",
    "    * [1D Roots](./Roots_1D.ipynb)\n",
    "    * [ND Roots](./Roots_ND.ipynb)\n",
    "* [Curve Fitting and Least Squares](./Curve_Fit.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "<img src=\"./images/global_local.svg\" style=\"float:right; margin: 2px\">\n",
    "\n",
    "Optimization algorithms find the smallest (or greatest) value of a function over a given range.  <b>Local</b> methods find a point that is just the lowest point for some <i>neighborhood</i>, but multiple local minima can exist over the domain of the problem.  <b>Global</b> methods try to find the lowest value over an entire region.\n",
    "\n",
    "\n",
    "The available local methods include `minimize_scalar`, `linprog`, and `minimize`. `minimize_scalar` only minimizes functions of one variable. `linprog` for <b>Linear Programming</b> deals with only linear functions with linear constraints.  `minimize` is the most general routine.  It can deal with either of those subcases in addition to any arbitrary function of many variables.\n",
    "\n",
    "Optimize provides 5 different global optimization routines.  Brute directly computes points on a grid. This routine is the easiest to get working, saving programmer time at the cost of computer time.  Conversely, shgo (Simplicial Homology Global Optimization) is a conceptually difficult but powerful routine.  Differential Evolution, Basin Hopping, and Dual Annealing are all [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) based, relying on random numbers.  Differential Evolution is an [Evolutionary Algorithm](https://en.wikipedia.org/wiki/Evolutionary_algorithm), creating a \"population\" of random points and iterating it based on which ones give the lowest values. Basin Hopping and Dual Annealing instead work locally through a [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) random walk.  They can computationally cope with a large number of dimensions, but they can get locally stuck and miss the global minimum.  Basin Hopping provides a more straightforward base and options for a great deal of customization, and dual annealing provides a more complicated method to avoid getting locally trapped.\n",
    "\n",
    "## Root Finding\n",
    "\n",
    "[`root`](./Roots_ND.ipynb) and [`root_scalar`](./Roots_1D.ipynb) solve the problem\n",
    "$$\n",
    "f(x) = 0.\n",
    "$$\n",
    "`root_scalar` is limited to when $x$ is a scalar, while for the more general `root` $x$ can be a multidimensional vector.\n",
    "\n",
    "## Curve Fitting\n",
    "\n",
    "This module provides tools to optimize the fit between a parametrized function and data.  `curve_fit` provides a simple interface where you don't have to worry about the guts of fitting a function.  It minimizes the sum of the residuals over the parameters of the function, or:\n",
    "$$\n",
    "\\text{min}_{\\text{p}} \\quad \\sum_i \\big( f(x_i , \\text{p} ) - y_i \\big)^2.\n",
    "$$\n",
    "If you want, you can instead pass this function to the provided non-linear least squares optimizer `least_squares`.  If the model function is linear, `nnls` and `lsq_linear` exist as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Traits in the Submodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output: `OptimizeResult`\n",
    "<hr />\n",
    "\n",
    "Many functions return an object that can contain more information than simply \"This is the minimium\".  The information varies between function, method used by the function, and flags given to function, but the way of accessing the data remains the same.  \n",
    "\n",
    "Let's create one of these data types via minimization to look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : x**2\n",
    "\n",
    "result=optimize.minimize(f,[2],method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can determine what data types are availible via "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'hess_inv', 'nfev', 'njev', 'status', 'success', 'message', 'x', 'nit'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can access individual values via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.88846401e-08])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the object with `?` or `??` can tell you more about what the individual components actually are.\n",
    "\n",
    "In Jupyter Lab, Contextual Help, `Ctrl+I` can also provide this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "? result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `args`\n",
    "<hr />\n",
    "\n",
    "Many routines allow function parameters in a <b>tuple</b> to be passed to the routine via the `args` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 5.5507662238258444e-17\n",
       " hess_inv: array([[0.5]])\n",
       "      jac: array([4.68181046e-13])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 9\n",
       "      nit: 2\n",
       "     njev: 3\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.99999999])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_parameter = lambda x,a : (x-a)**2\n",
    "\n",
    "optimize.minimize(f_parameter,[0],args=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "<hr />\n",
    "\n",
    "The functions in `scipy.optimize` are uniform wrappers that can call to multiple different methods, algorithms, behind the scenes.  For example, `minimize_scalar` can use Brent, Golden, or Bounded methods.  Methods can have different strengths, weaknesses, and pitfalls.  SciPy will automatically choose certain algorithms given inputted information, but if you know more about the problem, a different algorithm might be better.\n",
    "\n",
    "An example of choosing the routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 5.53784654790294e-15\n",
       "     jac: array([-1.33932256e-07])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 9\n",
       "     nit: 1\n",
       "    njev: 3\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([-7.44167088e-08])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x : x**2\n",
    "\n",
    "optimize.minimize(f,[2],method=\"CG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Options\n",
    "<hr />\n",
    "\n",
    "`minimize` itself has 14 different methods, and it's not the only routine that calls multiple methods.  While much of the information and functionality is unified across the routine, each method does have it's individual settings.  The settings can be found through the `show_options` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimization of scalar function of one or more variables using the\n",
      "conjugate gradient algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac` is approximated, use this value for the step size.\n"
     ]
    }
   ],
   "source": [
    "optimize.show_options(solver=\"minimize\",method=\"CG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings are passed in a dictionary to the solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 2.4972794524898593e-13\n",
       " hess_inv: array([[0.5]])\n",
       "      jac: array([5.4425761e-10])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 9\n",
       "      nit: 2\n",
       "     njev: 3\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-4.99727871e-07])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_dictionary = {\n",
    "    \"maxiter\": 5,\n",
    "    \"eps\": 1e-6\n",
    "}\n",
    "\n",
    "optimize.minimize(f,[2],options=options_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tolerance and Iterations\n",
    "<hr />\n",
    "\n",
    "How much computer time do you want to spend on this problem? How accurate do you need your answer? Is your function really expensive to calculate?\n",
    "\n",
    "When the two successive values are within the tolerance range of each other or the routine has reached the maximum number of iterations, the routine will exit.  Some functions differentiate between <b>relative tolerance</b> and absolute tolerance</b>.  Relative tolerance scales for the aboslute size of the values.  For example, if two steps are five apart, but each about a trillion, the function can exit. Tolerance in the domain `x` direction also differs from the tolerance in the range `f` direction.  For minimization, the `gtol` tolerance can also apply to zeroing the gradient.\n",
    "\n",
    "Some methods also allow for specifying both the maximum number of iterations and the maximum number of function evaluations.  Some methods evaulate a function multiple times during each iteration.\n",
    "\n",
    "Whether these quantities exist, and the procedure for setting these quantities varies between functions and methods within  functions.  Check individual documentation for details, but here is one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 5.55111515902901e-17\n",
       " hess_inv: array([[0.5]])\n",
       "      jac: array([-4.81884952e-17])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 12\n",
       "      nit: 3\n",
       "     njev: 4\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-7.45058062e-09])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize.minimize(f,[2],tol=1e-10,options={\"maxiter\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
